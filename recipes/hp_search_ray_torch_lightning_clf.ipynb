{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from base64 import encode\n",
    "from itertools import count\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from pydoc import doc\n",
    "from typing import List\n",
    "from black import out\n",
    "from matplotlib.pyplot import text\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import compute_class_weight\n",
    "import spacy\n",
    "import torch\n",
    "import typer\n",
    "from experiments.metadata.constraints.datasets.constraints_link.spacy_annoto_connectors import annoto2spacyDocs\n",
    "from origami_indexers.metadata.constraint_extraction.relation_component import get_possible_rels, rels_between_spans\n",
    "from origami_indexers.utils.pipes import NLP_CONTINGENCIES_PROPERTIES_DATASET_IMPORT_PIPES\n",
    "from origami_indexers.utils.spacy import NLP_DEFAULT\n",
    "from origami_indexers.utils.s3.s3 import LOCAL_CACHE_FOLDER, LOCAL_DATA_FOLDER, download_file_if_not_exists\n",
    "from origami_indexers.utils.s3.file_readers import s3_contingencies_annotations\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray import tune\n",
    "import torchmetrics\n",
    "\n",
    "docs_dump = LOCAL_DATA_FOLDER / 'annoto_docs_dump.pkl'\n",
    "lookup_table = dict(\n",
    "    c_severity_none=0,\n",
    "    c_severity_low=1,\n",
    "    c_severity_medium=2,\n",
    "    c_severity_high=3\n",
    ") \n",
    "classes = list(lookup_table.keys())\n",
    "\n",
    "\n",
    "SEVERITY_DATASET_FOLDER = LOCAL_DATA_FOLDER / 'severity'\n",
    "SEVERITY_DATASET_FOLDER.mkdir(exist_ok=True)\n",
    "SEVERITY_X_PATH = SEVERITY_DATASET_FOLDER / \"severity_x.bin\"\n",
    "SEVERITY_Y_PATH = SEVERITY_DATASET_FOLDER / \"severity_y.bin\"\n",
    "SEVERITY_CLASS_WEIGHTS_PATH = SEVERITY_DATASET_FOLDER / \"severity_class_weights.bin\"\n",
    "MAX_SEQ_LEN = 200\n",
    "\n",
    "\n",
    "#create_text_dataset_from_docs()\n",
    "#create_dataset_from_bert_embedding()\n",
    "# %%\n",
    "class SeverityDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = config['batch_size']\n",
    "        self.n_workers= 4    \n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download, split, etc...\n",
    "        # only called on 1 GPU/TPU in distributed\n",
    "        \n",
    "        x = torch.load(SEVERITY_X_PATH)\n",
    "        y = torch.load(SEVERITY_Y_PATH)\n",
    "\n",
    "        ss_split =  StratifiedShuffleSplit( test_size=0.33, random_state=42)\n",
    "        \n",
    "        train_idx, val_idx = next(ss_split.split(x,y))\n",
    "\n",
    "    \n",
    "        self.x_train = x[train_idx]\n",
    "        self.x_val = x[val_idx]\n",
    "        self.y_train = y[train_idx]\n",
    "        self.y_val = y[val_idx]\n",
    "    \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_split = TensorDataset(self.x_train, self.y_train)\n",
    "        return DataLoader(train_split, shuffle=True, batch_size=self.batch_size,num_workers=self.n_workers)\n",
    "    def val_dataloader(self):\n",
    "        val_split = TensorDataset(self.x_val, self.y_val)\n",
    "        return DataLoader(val_split,batch_size=self.batch_size, num_workers=self.n_workers)\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "class SeverityClf(pl.LightningModule):\n",
    "    def __init__(self, config, input_dim):\n",
    "        super(SeverityClf, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.dropout_rate = config[\"dropout_rate\"]\n",
    "        \n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "\n",
    "        # Input shape is (batch_size, seq_len,  n_dim)\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, \n",
    "                            hidden_size=config[\"lstm_hidden_size\"],\n",
    "                            num_layers=config[\"n_lstm_layers\"], \n",
    "                            dropout=self.dropout_rate, \n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(config[\"lstm_hidden_size\"], out_features=4)\n",
    "        self.loss = nn.CrossEntropyLoss(weight=config['weights'])\n",
    "        self.val_f1_weighted = torchmetrics.F1Score(num_classes=4, average='weighted')\n",
    "        self.val_precision_weighted = torchmetrics.Precision(num_classes=4, average='weighted')\n",
    "        self.val_recall_weighted = torchmetrics.Recall(num_classes=4, average='weighted')\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x, (hn,cn) = self.lstm(x)\n",
    "        # x :  batch_size, seq_len, lstm_hidden_size\n",
    "        # x is already the output of the last lstm layer\n",
    "        \n",
    "        x = self.linear(x[:,-1,:])  # take hidden state of last elt of seq\n",
    "\n",
    "\n",
    "        # x is raw output, not log\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        output = self.forward(x)\n",
    "        \n",
    "        loss = self.loss(output, y)\n",
    "        self.log(\"train/loss\", loss, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        output = self.forward(x)\n",
    "        loss = self.loss(output, y)\n",
    "        # Compute metrics\n",
    "        hard_labels = torch.argmax(output, dim=-1)\n",
    "        self.val_f1_weighted(hard_labels, y)\n",
    "        self.val_recall_weighted(hard_labels, y)\n",
    "        self.val_precision_weighted(hard_labels, y)\n",
    "        self.log('val/tr_loss', loss,on_step=False, on_epoch=True)\n",
    "        self.log('val/precision_weighted', self.val_precision_weighted, on_step=False, on_epoch=True)\n",
    "        self.log('val/recall_weighted', self.val_recall_weighted, on_step=False, on_epoch=True)\n",
    "        self.log('val/f1_weighted', self.val_f1_weighted, on_step=False, on_epoch=True)\n",
    "        # https://pytorch-lightning.readthedocs.io/en/stable/extensions/logging.html\n",
    "\n",
    "    \n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "def train_with_config(config, input_dim, num_gpus, enable_tune=True):\n",
    "    model = SeverityClf(config, input_dim)\n",
    "    tune_callback = TuneReportCallback([\"val/tr_loss\", \"val/f1_weighted\"], on=\"validation_end\")\n",
    "    callbacks = [tune_callback] if enable_tune else [pl.callbacks.progress.TQDMProgressBar()]\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config['epochs'],\n",
    "        gpus=num_gpus,\n",
    "        progress_bar_refresh_rate=0,\n",
    "        callbacks=callbacks)\n",
    "\n",
    "\n",
    "    trainer.fit(model, datamodule=SeverityDataModule(config))\n",
    "    return trainer\n",
    "\n",
    "\n",
    "@app.command()\n",
    "def hp_search(input_dim:int=768, num_samples:int=10, cpus_per_trial:int=1, gpus_per_trial:int=0,name='foo'):\n",
    "    \"\"\"\n",
    "    Run HP search with ray tune \n",
    "    \"\"\"\n",
    "    class_weights =  torch.load(SEVERITY_CLASS_WEIGHTS_PATH)\n",
    "    config = {\n",
    "        \"lstm_hidden_size\": tune.choice([64, 128]),\n",
    "        \"n_lstm_layers\": tune.choice([1,2]),\n",
    "        \"lr\": tune.loguniform(1e-5, 1e-3),\n",
    "        \"dropout_rate\": tune.uniform(0.1,0.4),\n",
    "        \"batch_size\": tune.choice([8, 16, 32]),\n",
    "        \"epochs\": tune.choice([5,10,20,40]),\n",
    "        \"weights\":class_weights\n",
    "    }\n",
    "\n",
    "    trainable = tune.with_parameters(\n",
    "        train_with_config,input_dim=input_dim, num_gpus=gpus_per_trial, enable_tune=True)\n",
    "    return tune.run(\n",
    "        trainable,\n",
    "        resources_per_trial={\n",
    "            \"cpu\": cpus_per_trial,\n",
    "            \"gpu\": gpus_per_trial\n",
    "        },\n",
    "        metric=\"val/tr_loss\",\n",
    "        search_alg='hyperopt',\n",
    "        mode=\"min\",\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        max_concurrent_trials=8,\n",
    "        name=name)\n",
    "\n",
    "#%%\n",
    "#analysis = hp_search(num_samples=n_samples, cpus_per_trial=1,gpus_per_trial=0,name=\"severity\")\n",
    "#%%\n",
    "\n",
    "# #%%\n",
    "# best_trial_config = analysis.get_best_trial(\"loss\", \"min\", \"last\").config\n",
    "# best_trial_config\n",
    "# # %%\n",
    "# best_trainer = train_with_config( best_trial_config, 29, national_df, disable_logging=True, num_gpus=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
